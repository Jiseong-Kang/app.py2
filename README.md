# ğŸŒ ë¹„ì²´ê³„ì  ìœ„í—˜ ë¶„ì„ Streamlit ì•± (ê³ ë„í™” ë²„ì „)
import streamlit as st
import requests
import re
import pandas as pd
import datetime
import plotly.express as px
from transformers import BertTokenizer, BertForSequenceClassification, pipeline
from sklearn.feature_extraction.text import TfidfVectorizer

# ê¸°ë³¸ ìœ„í—˜ í‚¤ì›Œë“œ ì •ì˜
BASE_RISK_KEYWORDS = {
    "ì¹¨ê³µ": 1.5, "ì „ìŸ": 1.4, "ë¹„íŒ": 1.2, "ë…¼ìŸ": 1.3, "ì œ3ì°¨ ì„¸ê³„ëŒ€ì „": 1.6,
    "ë¶„ì—´": 1.5, "ìœ„í˜‘": 1.4, "í›¼ì†": 1.6, "ì‹¤ìˆ˜": 1.2, "ë¶ˆë§Œ": 1.3,
    "ìœ„ê¸°": 1.4, "ìƒí™œë¹„": 1.1, "ë²”ì£„": 1.2, "ë¶ˆì•ˆê°": 1.3, "ê´€ì„¸": 1.3,
    "í•©ë³‘": 1.5,
    "recession": 1.5, "misleading": 1.3, "contracted": 1.4, "discontent": 1.3,
    "blamed": 1.1, "disrupted": 1.5, "scorn": 1.2, "failed": 1.6, "shortages": 1.3,
    "war": 1.7, "tariffs": 1.4, "levies": 1.2,
}

POSITIVE_KEYWORDS = {
    "í‰í™”": -0.8, "ë²ˆì˜": -0.6, "ì¬ê±´": -0.5, "ì„±ì¥": -0.4, "í˜‘ì •": -0.3,
    "íŒŒíŠ¸ë„ˆì‹­": -0.3, "ìœ ì¹˜": -0.4, "íšŒë³µ": -0.5, "ê¸°íšŒ": -0.4
}

@st.cache_resource
def load_sentiment_model():
    tokenizer = BertTokenizer.from_pretrained('monologg/kobert')
    model = BertForSequenceClassification.from_pretrained('beomi/kcbert-base', num_labels=3)
    return pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

def calculate_risk_score(text, keywords):
    score = 0
    for word, weight in keywords.items():
        count = len(re.findall(re.escape(word), text))
        score += weight * count
    return score

def kobert_sentiment(pipeline, text):
    result = pipeline(text[:512])[0]
    label = result['label']
    score = result['score']
    if label == 'LABEL_0': return -score
    elif label == 'LABEL_2': return score
    return 0

def extract_risk_keywords_tfidf(texts, top_n=20):
    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
    X = vectorizer.fit_transform(texts)
    tfidf_scores = X.sum(axis=0).A1
    feature_names = vectorizer.get_feature_names_out()
    tfidf_df = pd.DataFrame({'word': feature_names, 'score': tfidf_scores})
    top_keywords = tfidf_df.sort_values('score', ascending=False).head(top_n)
    return {row['word']: round(row['score'], 2) for _, row in top_keywords.iterrows()}

def visualize_keyword_contributions(text, keywords):
    contrib = []
    for k, w in keywords.items():
        count = text.lower().count(k)
        if count > 0:
            contrib.append({"í‚¤ì›Œë“œ": k, "ë¹ˆë„": count, "ê¸°ì—¬ë„": w * count})
    df = pd.DataFrame(contrib).sort_values("ê¸°ì—¬ë„", ascending=False)
    st.plotly_chart(px.bar(df, x="í‚¤ì›Œë“œ", y="ê¸°ì—¬ë„", text="ë¹ˆë„", title="ë¬¸ì„œ ë‚´ í‚¤ì›Œë“œë³„ ìœ„í—˜ ê¸°ì—¬ë„"))

# UI ì‹œì‘
st.set_page_config(page_title="Unsystematic Risk Analyzer", layout="wide")
st.title("ğŸ“‰ ë‰´ìŠ¤ ê¸°ë°˜ ë¹„ì²´ê³„ì  ìœ„í—˜ ë¶„ì„ê¸°")

keyword = st.text_input("ğŸ” ë‰´ìŠ¤ í‚¤ì›Œë“œ", "ì•„ë¥´í—¨í‹°ë‚˜ ì •ì¹˜ ë¶ˆì•ˆ")
api_key = st.text_input("ğŸ”‘ GNews API Key", type="password")
custom_article = st.text_area("âœï¸ ë¶„ì„í•  ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥ (ì„ íƒ)", height=300)

# í‚¤ì›Œë“œ ì„¤ì •
st.sidebar.title("âš™ï¸ í‚¤ì›Œë“œ ì„¤ì •")
keyword_mode = st.sidebar.radio("í‚¤ì›Œë“œ ì„¤ì • ë°©ì‹", ["ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©", "ì§ì ‘ ì¶”ê°€", "TF-IDF ê¸°ë°˜ ìë™ ì¶”ì¶œ"])
custom_risks = {}

if keyword_mode == "ì§ì ‘ ì¶”ê°€":
    with st.sidebar.expander("ìœ„í—˜ í‚¤ì›Œë“œ ì¶”ê°€"):
        new_kw = st.text_input("í‚¤ì›Œë“œ ì…ë ¥")
        new_weight = st.number_input("ê°€ì¤‘ì¹˜", 0.1, 2.0, step=0.1)
        if st.button("â• ì¶”ê°€"):
            custom_risks[new_kw] = new_weight
            st.success(f"ì¶”ê°€ë¨: {new_kw} ({new_weight})")
    keywords = BASE_RISK_KEYWORDS.copy()
    keywords.update(custom_risks)
else:
    keywords = BASE_RISK_KEYWORDS.copy()

# ë¶„ì„ ì‹œì‘
if st.button("ğŸš€ ë¶„ì„ ì‹œì‘"):
    with st.spinner("ëª¨ë¸ ë¡œë”© ì¤‘..."):
        nlp = load_sentiment_model()

    if custom_article.strip():
        articles = [custom_article.strip()]
    else:
        url = f"https://gnews.io/api/v4/search?q={keyword}&lang=ko&token={api_key}"
        r = requests.get(url)
        articles = [a['title'] + ' ' + (a['description'] or '') for a in r.json().get('articles', [])]

    if not articles:
        st.warning("ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        if keyword_mode == "TF-IDF ê¸°ë°˜ ìë™ ì¶”ì¶œ":
            keywords = extract_risk_keywords_tfidf(articles)
            st.sidebar.success("TF-IDF í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ ì™„ë£Œ!")

        rows = []
        for t in articles:
            base_score = calculate_risk_score(t, keywords)
            bonus = calculate_risk_score(t, POSITIVE_KEYWORDS)
            final_score = base_score + bonus
            polarity = kobert_sentiment(nlp, t)
            score = round(final_score * (1 - polarity), 2)
            rows.append({"í…ìŠ¤íŠ¸": t, "ìœ„í—˜ ì ìˆ˜": final_score, "ê°ì„± ì ìˆ˜": round(polarity, 2), "ìµœì¢… ìœ„í—˜ ì§€ìˆ˜": score,
                         "ì‹œê°„": datetime.datetime.now()})

        df = pd.DataFrame(rows)
        st.success(f"ì´ {len(df)}ê°œ ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ!")
        st.dataframe(df.sort_values("ìµœì¢… ìœ„í—˜ ì§€ìˆ˜", ascending=False))

        st.subheader("ğŸ“Š ì‹œê°í™” ê²°ê³¼")
        st.plotly_chart(px.line(df, x="ì‹œê°„", y="ìµœì¢… ìœ„í—˜ ì§€ìˆ˜", title="ì‹œê°„ë³„ ìœ„í—˜ ì¶”ì„¸"))
        st.plotly_chart(px.histogram(df, x="ìµœì¢… ìœ„í—˜ ì§€ìˆ˜", nbins=15, title="ìœ„í—˜ ë¶„í¬"))
        st.plotly_chart(px.scatter(df, x="ê°ì„± ì ìˆ˜", y="ìœ„í—˜ ì ìˆ˜", color="ìµœì¢… ìœ„í—˜ ì§€ìˆ˜", size="ìµœì¢… ìœ„í—˜ ì§€ìˆ˜",
                                   hover_data=["í…ìŠ¤íŠ¸"], title="ê°ì„± vs ìœ„í—˜ ì ìˆ˜ ì‚°ì ë„"))

        top5 = df.sort_values("ìµœì¢… ìœ„í—˜ ì§€ìˆ˜", ascending=False).head(5)
        st.subheader("ğŸ”¥ ê³ ìœ„í—˜ ê¸°ì‚¬ Top 5")
        for i, row in top5.iterrows():
            st.markdown(f"**{i+1}. ìœ„í—˜ë„: {row['ìµœì¢… ìœ„í—˜ ì§€ìˆ˜']} / ê°ì„±: {row['ê°ì„± ì ìˆ˜']}**")
            st.caption(row['í…ìŠ¤íŠ¸'][:400] + ("..." if len(row['í…ìŠ¤íŠ¸']) > 400 else ""))

        if st.checkbox("ë¬¸ì„œë³„ í‚¤ì›Œë“œ ê¸°ì—¬ë„ ë³´ê¸°"):
            for i, row in df.iterrows():
                st.markdown(f"**ë¬¸ì„œ {i+1}**")
                st.caption(row['í…ìŠ¤íŠ¸'][:300] + "...")
                visualize_keyword_contributions(row['í…ìŠ¤íŠ¸'], keywords)

        st.download_button("ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode(), "risk_analysis.csv", "text/csv")
